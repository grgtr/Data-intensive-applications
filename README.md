# Data-intensive applications

## I Модель отказов распределенной системы, репликация и шардирование (определение и причины), способы решардирования

#### Типы отказов
 - **Crash failure** Узел перестает функционировать, не отправляет сообщения и не отвечает.
  - **Limited failure** Узел ведет себя произвольно, включая отправку ложных данных.
  - **Network partition** Соединение между узлами прерывается, что приводит к разделению сети. Граф серверов распался на компоненты, в которых сохранились разные версии одних и тех же данных. После восстановления связей не понятно, какие данные должны стать актуальными.
  - **Latency** Задержка в доставке сообщений, не гарантирующая своевременную синхронизацию.
  - **Skew** Сдвиг, возникает разница между часами на нодах.

####  Репликация и шардирование (определение и причины):
 - **Репликация** — это создание копий данных или процессов для обеспечения отказоустойчивости, доступности и улучшения производительности.
 - **Шардирование** - это разбиение данных на фрагменты (шарды), которые распределяются между узлами.
    Причины:
     - Масштабируемость: Данные равномерно распределяются между узлами.
     - Улучшение производительности: Параллельная обработка запросов.
     - Устойчивость к отказам: Отказ одного шарда затрагивает только его данные.


####  Способы решардирования
Проблемы возникают при появлении нового шарда, придётся переносить на него часть старых данных для равномерного распределения. Поэтому существуют shard managers.
 - Можно создать сабшарды, если мы знаем верхнее ограничение на количество шардов.
 - Консистентное хеширование. Представим кольцо от 0 до INTMAX. Отметим на нём хеши айдишников шардов. Каждый раз, добавляя ключ, берём его хеш, ищем его на кольце и находим ближайший по часовой стрелке шард. Добавляем новый шард -- просто отмечаем его хеш на кольце. При хорошей хеш функции данные распределяются примерно равномерно.Ноды удобно будет хранить в дереве поиска.

## II RPC, TCP, сериализация (protobuf, json)

 - **RPC** -  Remote procedure call. Удалённо вызываем методы серверного кода.
 - **TCP** -  Transmission Control Protocol. Транспортный протокол, обеспечивающий надежную доставку данных в сети.
    - Гарантирует доставку всех пакетов в правильном порядке.
    - Использует подтверждения (ACK) и повторные передачи.

 - Cериализация (protobuf, json):
    -   **Protobuf** - Разработан Google. Бинарный формат: компактный и быстрый.
        Требует предварительного определения схемы (описание структуры данных в .proto-файле).
        ```
        message User {
            int32 id = 1;
            string name = 2;
        }
        ```
    - **Json** - Читаемый для человека текстовый формат. Меньшая производительность по сравнению с бинарными форматами. Удобен для систем, где важна совместимость и простота.
    - **XML** - Читаемый текстовый формат, но избыточный и медленный. Устаревает в пользу JSON и Protobuf.

## III Основные характеристики RAM, дисков и сети. Модель внешней памяти, базовые виды задач

|         | size   | latency (DC / cross DC) | throughput |
| ------- | ------ | ----------------------- | ---------- |
| Network | ---    | 1 ms / 10 ms            | 1 Gb / s   |
| RAM     | 1 Tb   | 100 ns                  | 10 Gb / s  |
| HDD     | 100 Tb | 10 ms                   | 1 Gb / s   |
| SSD     | 100 Tb | 100 micro s             | 10 Gb / s  |
#### RAM
- Лента. Минимальный размер записи порядка байта.
- Характеристики:
   - Производительность. Время доступа: 100 ns. Пропускная способность: десятки гигабайт в секунду.
    - Объем ограничен стоимостью и архитектурными особенностями (например, 16–128 ГБ на сервере).
    - Данные, расположенные близко друг к другу, читаются быстрее. Повторное использование данных ускоряет доступ.
#### HDD
- Крутящийся диск. Минимальый размер записи порядка 512 байт.
- Характеристики:
    - Время доступа: миллисекунды 10 ms.
    - Используется для архивирования и долгосрочного хранения данных.

#### SSD
- В блоке порядка 512 страниц.
- Характеристики:
    - Время доступа: микросекунды (10–100 мкс).
    - Высокая скорость чтения/записи
    - Подходит для высокопроизводительных систем.
    - Имеет ограниченное количество циклов записи.

#### Сеть
- Пропускная способность:
    - Количество данных, которое может быть передано за единицу времени (например, 1 Гбит/с).
    - Ограничивает скорость обмена между узлами в распределенных системах.
- Задержка (latency):
    - Время, необходимое для передачи пакета от одного узла к другому.
    - Типичные значения: десятки миллисекунд в WAN, миллисекунды в локальных сетях.
- Пакетные потери:
  - Приводят к повторным передачам и увеличению времени выполнения операций.


#### Модель внешней памяти
Модель внешней памяти (External Memory Model) используется для оценки алгоритмов, которые работают с данными, превышающими объем оперативной памяти.

 - Основная идея:
    - Модель фокусируется на количестве операций ввода/вывода (I/O), так как они гораздо дороже операций в оперативной памяти.
    - Данные хранятся в блоках фиксированного размера (например, 4 КБ).
 - Пример:
   -  B-tree: Многовершинная структура, минимизирующая количество операций I/O.
    - External Merge Sort: Эффективный алгоритм сортировки больших данных, использующий несколько этапов слияния.

#### Базовые виды задач
1. Сортировка внешней памяти.
Пример: External Merge Sort. Использует минимальное количество операций I/O.
2. Поиск.
Используются структуры, оптимизированные для I/O (B-tree, LSM-tree).
3. Обработка больших данных:
Распределенные системы (Hadoop, Spark) эффективно используют дисковую и сетевую подсистемы для работы с данными.
4. Кэширование:
Позволяет уменьшить количество обращений к дискам, используя принципы локальности.

## IV [ExtSort](https://en.wikipedia.org/wiki/External_sorting)
**External sorting** - это алгоритм сортировки, разработанный для обработки данных, которые не помещаются в оперативную память и хранятся на диске или других устройствах внешней памяти. Основная цель — минимизация количества операций ввода-вывода (I/O), так как они являются самым затратным этапом работы с внешней памятью.

Aлгоритмы внешней сортировки можно анализировать в модели внешней памяти . В этой модели кэш или внутренняя память размера **M** и неограниченная внешняя память разделены на блоки размера **B** , а время работы алгоритма определяется количеством передач памяти между внутренней и внешней памятью. Как и их аналоги , не обращающие внимания на кэш , асимптотически оптимальные алгоритмы внешней сортировки достигают времени работы  $${\displaystyle O\left({\tfrac {N}{B}}\log _ {\tfrac {M}{B}}{\tfrac {N}{B}}\right)}$$. 

## V B-tree
 [B-tree итмо](https://neerc.ifmo.ru/wiki/index.php?title=B-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE)
Система в состоянии поддерживать в процессе работы в оперативной памяти только ограниченное количество страниц. Мы будем считать, что страницы, которые более не используются, удаляются из оперативной памяти системой; наши алгоритмы работы с В-деревьями не будут заниматься этим самостоятельно. Поскольку в большинстве систем время выполнения алгоритма, работающего с В-деревьями, зависит в первую очередь от количества выполняемых операций чтения/записи с диском, желательно минимизировать их количество и за один раз считывать и записывать как можно больше информации. Таким образом, размер узла В-дерева обычно соответствует дисковой странице. Количество потомков узла В-дерева, таким образом, ограничивается размером дисковой страницы. 


## VI Buffered tree
В структуру ноды добавляется ещё буфер, туда пишутся операции на будущее вставить/удалить, когда буфер ноды переполнился, то спускаем его в детей
Для поиска сначала обрабатываются все операции в буфере текущего узла.
Затем поиск продолжается в соответствующем дочернем узле.
[Статья The Buffer Tree: A New Technique for Optimal I/O Algorithms](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5353ed87f7ec15c32d66f81a0ad9ba2f695f0855)
[Валера про Buffered tree](https://gitlab.com/saltysallysmine/mipt/-/blob/master/%D0%A1%D0%B5%D0%BC%D0%B5%D1%81%D1%82%D1%80-5/3-%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%91%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B/Lectures/2024-09-27-Buffered-tree.md?ref_type=heads)

#### Анализ производительности
Обозначения:
- N — количество элементов.
- B — размер страницы ввода-вывода.

Вставка/удаление:
- Каждая операция перемещается по дереву до листа.
- В среднем требуется:
  - $${ O(\tfrac {logN} {logB})}$$ операций ввода-вывода.

Поиск: 
- Сложность аналогична вставке:
  - $${ O(\tfrac {logN} {logB})}$$.

## VII LSM, способы реализации операции compaction
В RAM храним сортированную структуру, например, красно-черное дерево. Когда она
переполняется, флашим её на диск в новый отсортированный файл, он называется
ss-table (или chunk). После нескольких флашей получается куча упорядоченных,
неизменяемых файлов (возможно, разного размера) на диске (потому что изменять
их - вставлять новые данные потенциально в центр - долгая операция)
Чтобы быстро узнавать, есть ли ключ на диске, построим фильтр Блума
(вероятностная структура, отвечает "да" на все присутствующие).

#### Иерархия размеров ss-таблиц.

**Tier** - ограничение, какого размера может быть чанк.

Деления на тиры имеют размеры $${ \alpha, \alpha ^ 2, ..., \alpha ^ k }$$.

Когда мы сливаем несколько маленьких чанков, может получиться так, что они не
перейдут на следующий тир, об этом нужно помнить.

> [!NOTE] Благодаря степеням альфа получается так, что в каждом тире условно
> менше T чанков.

Флаш всегда идёт в самый младший чанк.

```
Read Amplification = RA = #tiers * T

Write Amplification = WA = 
```

Когда сливаем (делаем компакшн), переносим в следующий тир самые старые записи,
чтобы новые значения всегда находились первыми и на них можно было
останавливаться при look-up'е.

---

Второй вариант – **levels**.

Расположим ключи из чанков на числовой прямой. Если два чанка пересекаются,
смрёрджим их. Если переполнение – переносим чанк на уровень выше (с L_i на
L_(I+1)).

```
Read Amplification = RA = #levels + #L_0 chunks

Write Amplification = WA = #levels * k
```

**В тирах – плохо чтение. В левелах – запись.**

**Компакция** — это процесс слияния данных между уровнями для:
   - Удаления дубликатов.
    - Применения записей tombstone.
    - Упорядочивания данных на уровне.

## VIII Bloom filter

Есть k хеш-функций. При добавлении элемента вычисляем все хеш-функци,
закрашиваем клетки, в которые они указывают.
Поэтому если при запросе contains хотя бы одна из клеток, на которую указывает
хеш, не закрашена, элемента точно нет.

Для проверки элемента y, хэш-функции вычисляют индексы h1(y),h2(y),…,hk(y)
Если все соответствующие биты равны 1, то фильтр говорит, что элемент, возможно, принадлежит множеству.
Если хотя бы один бит равен 0, то элемент точно не принадлежит множеству.

Bloom Filter часто используется для оптимизации операций поиска в LSM-Tree:

- Ускорение поиска:
  - Каждое SSTable (Sorted String Table) снабжается своим Bloom Filter.
  - Перед чтением SSTable проверяется Bloom Filter.
  - Если фильтр говорит, что элемента нет, чтение SSTable пропускается.
- Снижение количества I/O:
    - Если множество SSTables велико, фильтр уменьшает количество обращений к диску.


## IX Persistence: Полный протокол записи блоков на диск

Пусть в f лежит `456`.

`write(/d/f, offset, "123")`.

Какие проблемы могут возникнуть?

1. Restart after writing.

    Записали 123, однако случился рестарт и в файле новых
    данных не оказалось. Почему? Мы написали 123 в _page cache_ и он не успел
    зафлашиться в память.

    Solution: syscall `fsync(/d/f)`.

2. Restart in the middle of writing (partial write). В файле может оказаться 126
   или 423 (запись не обязана быть последовательной).

    Solution: redo log

    ```cpp
     create("/d/log");
     write("/d/log", {"d/f", offset, "123"});
     write("/d/f", offset, "123");
     fsync("/d/f");
     remove("/d/log");
    ```

3. Partial log write.
    
    Solution: checksum for log, magick number (кладём в конец записи какую-то
    константу. Если при перечитывании лога видим, что вместо магического числа
    лежит белиберда, значит, запись точно не удалась).

4. File & log corruption.

    Solution: `fsync("/d/log")`

    ```cpp
     create("/d/log");

     write("/d/log", {"d/f", offset, checksum, "123"});
     fsync("/d/log");

     write("/d/f", offset, "123");
     fsync("/d/f");

     remove("/d/log");
    ```

    Теперь если падаем до синка лога, ничего в файл не будет записано, у нас
    получается транзация: либо всё, либо ничего.

5. Lost redo log.

    `create` - тоже буферизированная операция. Информация о том, что лог
    существует лежит в RAM (и содержится в папке "/d"), но не записывается на
    диск сразу.

    Solution: `fsync("/d")`.

    ```cpp
     create("/d/log");
     fsync("/d");

     write("/d/log", {"d/f", offset, checksum, "123"});
     fsync("/d/log");

     write("/d/f", offset, "123");
     fsync("/d/f");

     remove("/d/log");
     fsync("/d");
    ```

6. fsync error.

    Долгое время в Постгресе часто выскакивала ошибка fsync, поэтому они
    заворачивали fsync в while. Этого делать нельзя :)

    Кто-то вынул флешку, первый синк выдал ошибку в середине записи, а потом мы
    попробовали ретрай. Синк посмотрел - устройства не существует - и выдал "0
    байт записано".

    Решения нет, **ретрай делать нельзя**.

#### Checksums

1. RAM.
2. Disk.
3. Network.


## X Paxos

[Валера про Paxos](https://gitlab.com/saltysallysmine/mipt/-/blob/master/%D0%A1%D0%B5%D0%BC%D0%B5%D1%81%D1%82%D1%80-5/3-%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%91%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B/Lectures/2024-10-25-Replication.md?ref_type=heads)

**Paxos** — это алгоритм для достижения распределенного консенсуса в системах с ненадежной коммуникацией и узлами. Его основная задача — гарантировать, что группа узлов (distributed system) согласует одно значение, даже если некоторые из узлов могут быть недоступны или сообщения между ними теряются.

Google Spanner использует Paxos для согласования данных между репликами.

## XI Multi-paxos
[Валера про Multi-Paxos](https://gitlab.com/saltysallysmine/mipt/-/blob/master/%D0%A1%D0%B5%D0%BC%D0%B5%D1%81%D1%82%D1%80-5/3-%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%91%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B/Lectures/2024-11-08-Multi-Paxos.md?ref_type=heads)

**Multi-Paxos** — это расширение базового Paxos, предназначенное для согласования не одного, а последовательности значений в распределенной системе. Эта оптимизация делает Paxos более эффективным для систем, в которых требуется согласовать множество операций, таких как распределенные журналы (лог операций) или базы данных.

**Основные отличия от базового Paxos**
- Повторное использование лидерства:
    - Multi-Paxos минимизирует количество фаз Prepare, позволяя лидеру (Proposer) согласовывать новые значения без необходимости повторного получения согласия (Promise) от Acceptors.
- Согласование последовательности значений:
    - Вместо согласования одного значения в одном раунде, Multi-Paxos позволяет согласовывать множество значений в рамках одной и той же сессии лидера.
- Оптимизация коммуникации:
    - Лидер может сразу переходить к фазе Accept для новых операций, если большинство узлов уже согласились с его лидерством.

**Ключевые аспекты Multi-Paxos**
- Лидерство:
    - Один Proposer становится лидером (Leader) и сохраняет эту роль до сбоя или выборов нового лидера.
    Лидер несет ответственность за выбор и согласование новых значений.
 - Журнал операций (Log):
    - Значения, согласованные в Multi-Paxos, записываются в лог, где каждый индекс лога соответствует отдельному раунду консенсуса.
- Кворум:
    - Для согласования значения требуется ответ большинства Acceptors, аналогично базовому Paxos.

**Алгоритм Multi-Paxos**
Инициализация
- Узлы договариваются о новом лидере, используя фазу Prepare (как в базовом Paxos).Лидер получает право предлагать новые значения без повторного выполнения фазы Prepare.

Фаза 1: Выбор лидера
- Proposer отправляет запрос Prepare(n) всем Acceptors.
- Acceptors отвечают Promise(n), подтверждая, что они больше не будут принимать предложения с номером меньше nn.
- Если лидер получает большинство Promise, он становится активным лидером.

Фаза 2: Accept для новых операций
- Лидер отправляет запросы Accept(n, v) для согласования новых значений vv на индексах лога.
- Acceptors проверяют, что номер предложения nn больше или равен последнему обработанному предложению, и принимают vv.

Фаза 3: Logs
- Как только значение согласовано большинством, оно становится фиксированным (committed).
- Узлы узнают согласованное значение, и оно добавляется в журнал операций.

## XI Multi-paxos: оптимизации, sloppy quorums
#### Оптимизации Multi-Paxos
- Batching:
    Лидер может согласовывать сразу несколько операций в одном запросе Accept.
- Сохранение состояния:
    Узлы сохраняют свои журналы на диск, чтобы после перезапуска продолжить работу с последнего согласованного значения.
- Пропуск пустых слотов:
    Если некоторые индексы лога остались пустыми (например, из-за сбоя), лидер может пропустить их, предложив "no-op" значения.

#### Sloppy Quorums

**Sloppy Quorums** — это подход, позволяющий системе продолжать работать в условиях высокой нагрузки или временных сбоев, при этом не строго соблюдая стандартные требования кворума. В отличие от строгого кворума, где большинство узлов должны участвовать в операции, Sloppy Quorums позволяют временно ослабить это требование.

**Ключевые идеи Sloppy Quorums**

- Ослабленный кворум:
    Вместо большинства узлов (⌈N/2⌉+1⌈N/2⌉+1), система допускает временную работу с меньшим количеством узлов.

- Требования к безопасности (Safety):
    Согласованность может временно нарушаться, но система гарантирует, что данные восстановятся при нормализации сети.

- Требования к живучести (Liveness):
    Узлы продолжают принимать операции даже при потере связи с большинством.

## XIII RAFT

**RAFT** — это алгоритм консенсуса, используемый для согласования распределенного журнала операций (log replication) между узлами в кластерной системе. Его основная цель — быть проще в понимании и реализации, чем Paxos, при этом обеспечивая те же гарантии согласованности и живучести.

#### Основные роли узлов

- Лидер (Leader):
    - Единственный узел, принимающий клиентские запросы.
    - Рассылает операции для репликации на другие узлы.
    - Следит за их состоянием.
- Кандидат (Candidate):
    - Узел, который инициирует выборы лидера, если текущий лидер недоступен.
- Последователь (Follower):
    - Узел, который пассивно принимает запросы на репликацию от лидера.

#### Этапы работы RAFT
1. Выбор лидера
    - Выбор лидера происходит при следующих условиях:
        - Лидер выходит из строя.
        - Последователь не получает от лидера сообщений AppendEntries в течение таймаута.
    - Шаги:
         - Узел становится кандидатом и увеличивает текущий term (эпоху).
         - Кандидат отправляет запросы RequestVoteRequestVote всем узлам.
         - Узлы голосуют за кандидата, если:
            Они еще не голосовали за другого кандидата в текущем term.
            Журнал кандидата не отстает от их собственного (обеспечение согласованности).
         - Кандидат становится лидером, если он получает большинство голосов.

2. Репликация журнала
Лидер отвечает за запись операций в распределенный журнал.
Шаги:
    - Клиент отправляет запрос на запись данных лидеру.
    - Лидер добавляет операцию в свой журнал и отправляет AppendEntries всем последовательным узлам.
    - Последователи записывают операцию в свои журналы и отправляют подтверждения лидеру.
    - Как только большинство узлов подтвердят запись, операция считается зафиксированной (committed).

3. Обработка отказов
- Сбой лидера:
    Если узлы не получают AppendEntries от лидера в течение таймаута, они начинают выборы.

- Сбой последовательного узла:
    Лидер продолжает попытки синхронизации с узлом, когда он восстанавливается.

- Расхождение журналов:
    Лидер исправляет расхождение журналов, удаляя или перезаписывая записи, которые не согласованы.

## XIV KV хранилище: алгоритм + дизайн (шардирование, устройство на уровне отдельного узла)
`set(k, v)`
`get(k)`
Чтобы не было проблем с актуальностью данных будем записывать вместе с
таймстемпом. Иначе выйдет ситуация, что два юзера пишут по одному ключу разные
данные, и на один узел раньше приходят данные первого юзера, а на второй --
второго, и мы не сможем этого определить.
Под таймстемпом подразумевается пара (record version, real timestamp).

Get
Пусть одновременно происходит set(1) and get, дальше ещё один get. Пусть
изначальное значение 0. Будет нехорошо, если get_1 = 1, get_2 = 0 (нарушается
линеаризуемость). Это может произойти, если до каких-то машин set дойдёт раньше
get_1, до каких-то позже.
Поэтому будем делать внутри каждого get set, содержащий в точности то значение,
которое вытащил get (с той же версией записи и таймстемпом).

Гарантируем линеаризацию (поступающие операции упорядочены)


































